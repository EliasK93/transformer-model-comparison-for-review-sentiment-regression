2022-01-05 12:40:41,521 ----------------------------------------------------------------------------------------------------
2022-01-05 12:40:41,521 Model: "TextRegressor(
  (document_embeddings): TransformerDocumentEmbeddings(
    (model): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=1, bias=True)
  (loss_function): MSELoss()
)"
2022-01-05 12:40:41,536 ----------------------------------------------------------------------------------------------------
2022-01-05 12:40:41,536 Corpus: "Corpus: 4275 train + 475 dev + 250 test sentences"
2022-01-05 12:40:41,536 ----------------------------------------------------------------------------------------------------
2022-01-05 12:40:41,536 Parameters:
2022-01-05 12:40:41,536  - learning_rate: "5e-05"
2022-01-05 12:40:41,536  - mini_batch_size: "2"
2022-01-05 12:40:41,536  - patience: "3"
2022-01-05 12:40:41,536  - anneal_factor: "0.5"
2022-01-05 12:40:41,536  - max_epochs: "10"
2022-01-05 12:40:41,536  - shuffle: "True"
2022-01-05 12:40:41,536  - train_with_dev: "False"
2022-01-05 12:40:41,536  - batch_growth_annealing: "False"
2022-01-05 12:40:41,536 ----------------------------------------------------------------------------------------------------
2022-01-05 12:40:41,536 Model training base path: "models\roberta-base_fine_tuned"
2022-01-05 12:40:41,536 ----------------------------------------------------------------------------------------------------
2022-01-05 12:40:41,536 Device: cuda:0
2022-01-05 12:40:41,536 ----------------------------------------------------------------------------------------------------
2022-01-05 12:40:41,536 Embeddings storage mode: gpu
2022-01-05 12:40:41,536 ----------------------------------------------------------------------------------------------------
2022-01-05 12:41:26,581 epoch 1 - iter 213/2138 - loss 4.34775334 - samples/sec: 12.21 - lr: 0.000005
2022-01-05 12:42:02,310 epoch 1 - iter 426/2138 - loss 2.81571613 - samples/sec: 12.04 - lr: 0.000010
2022-01-05 12:42:37,700 epoch 1 - iter 639/2138 - loss 2.21151987 - samples/sec: 12.14 - lr: 0.000015
2022-01-05 12:43:13,405 epoch 1 - iter 852/2138 - loss 1.93960550 - samples/sec: 12.04 - lr: 0.000020
2022-01-05 12:43:47,348 epoch 1 - iter 1065/2138 - loss 1.74824354 - samples/sec: 12.67 - lr: 0.000025
2022-01-05 12:44:22,291 epoch 1 - iter 1278/2138 - loss 1.61411680 - samples/sec: 12.33 - lr: 0.000030
2022-01-05 12:44:56,505 epoch 1 - iter 1491/2138 - loss 1.51779528 - samples/sec: 12.54 - lr: 0.000035
2022-01-05 12:45:32,204 epoch 1 - iter 1704/2138 - loss 1.47327567 - samples/sec: 12.03 - lr: 0.000040
2022-01-05 12:46:08,480 epoch 1 - iter 1917/2138 - loss 1.44901851 - samples/sec: 11.85 - lr: 0.000045
2022-01-05 12:46:45,098 epoch 1 - iter 2130/2138 - loss 1.43094896 - samples/sec: 11.73 - lr: 0.000050
2022-01-05 12:46:46,916 ----------------------------------------------------------------------------------------------------
2022-01-05 12:46:46,916 EPOCH 1 done: loss 3060.7849 - lr 0.0000500
2022-01-05 12:47:07,391 DEV : loss 0.4671526849269867 - f1-score (micro avg)  0.7951
2022-01-05 12:47:08,810 BAD EPOCHS (no improvement): 4
2022-01-05 12:47:08,810 ----------------------------------------------------------------------------------------------------
2022-01-05 12:47:55,363 epoch 2 - iter 213/2138 - loss 1.09658425 - samples/sec: 11.91 - lr: 0.000049
2022-01-05 12:48:30,246 epoch 2 - iter 426/2138 - loss 1.06318815 - samples/sec: 12.31 - lr: 0.000049
2022-01-05 12:49:05,744 epoch 2 - iter 639/2138 - loss 1.02106037 - samples/sec: 12.10 - lr: 0.000048
2022-01-05 12:49:40,458 epoch 2 - iter 852/2138 - loss 0.99331244 - samples/sec: 12.38 - lr: 0.000048
2022-01-05 12:50:17,272 epoch 2 - iter 1065/2138 - loss 0.95377686 - samples/sec: 11.67 - lr: 0.000047
2022-01-05 12:50:52,020 epoch 2 - iter 1278/2138 - loss 0.95237881 - samples/sec: 12.38 - lr: 0.000047
2022-01-05 12:51:26,915 epoch 2 - iter 1491/2138 - loss 0.94465403 - samples/sec: 12.30 - lr: 0.000046
2022-01-05 12:52:02,855 epoch 2 - iter 1704/2138 - loss 0.91106742 - samples/sec: 11.98 - lr: 0.000046
2022-01-05 12:52:35,898 epoch 2 - iter 1917/2138 - loss 0.89164682 - samples/sec: 13.00 - lr: 0.000045
2022-01-05 12:53:11,643 epoch 2 - iter 2130/2138 - loss 0.88331278 - samples/sec: 12.04 - lr: 0.000044
2022-01-05 12:53:13,215 ----------------------------------------------------------------------------------------------------
2022-01-05 12:53:13,215 EPOCH 2 done: loss 1887.2962 - lr 0.0000444
2022-01-05 12:53:33,704 DEV : loss 0.3149796426296234 - f1-score (micro avg)  0.8421
2022-01-05 12:53:35,214 BAD EPOCHS (no improvement): 4
2022-01-05 12:53:35,214 ----------------------------------------------------------------------------------------------------
2022-01-05 12:54:20,146 epoch 3 - iter 213/2138 - loss 0.56490653 - samples/sec: 12.42 - lr: 0.000044
2022-01-05 12:54:55,809 epoch 3 - iter 426/2138 - loss 0.62261599 - samples/sec: 12.06 - lr: 0.000043
2022-01-05 12:55:31,623 epoch 3 - iter 639/2138 - loss 0.60385717 - samples/sec: 11.97 - lr: 0.000043
2022-01-05 12:56:08,575 epoch 3 - iter 852/2138 - loss 0.64250997 - samples/sec: 11.64 - lr: 0.000042
2022-01-05 12:56:42,691 epoch 3 - iter 1065/2138 - loss 0.67189278 - samples/sec: 12.62 - lr: 0.000042
2022-01-05 12:57:18,640 epoch 3 - iter 1278/2138 - loss 0.68458295 - samples/sec: 11.95 - lr: 0.000041
2022-01-05 12:57:55,430 epoch 3 - iter 1491/2138 - loss 0.67871087 - samples/sec: 11.66 - lr: 0.000041
2022-01-05 12:58:30,826 epoch 3 - iter 1704/2138 - loss 0.68077115 - samples/sec: 12.14 - lr: 0.000040
2022-01-05 12:59:04,713 epoch 3 - iter 1917/2138 - loss 0.68326334 - samples/sec: 12.64 - lr: 0.000039
2022-01-05 12:59:40,052 epoch 3 - iter 2130/2138 - loss 0.67781062 - samples/sec: 12.19 - lr: 0.000039
2022-01-05 12:59:41,945 ----------------------------------------------------------------------------------------------------
2022-01-05 12:59:41,945 EPOCH 3 done: loss 1447.5304 - lr 0.0000389
2022-01-05 13:00:02,485 DEV : loss 0.35210156440734863 - f1-score (micro avg)  0.865
2022-01-05 13:00:03,963 BAD EPOCHS (no improvement): 4
2022-01-05 13:00:03,963 ----------------------------------------------------------------------------------------------------
2022-01-05 13:00:49,312 epoch 4 - iter 213/2138 - loss 0.49046116 - samples/sec: 12.32 - lr: 0.000038
2022-01-05 13:01:25,649 epoch 4 - iter 426/2138 - loss 0.52163278 - samples/sec: 11.80 - lr: 0.000038
2022-01-05 13:02:01,510 epoch 4 - iter 639/2138 - loss 0.49690030 - samples/sec: 11.98 - lr: 0.000037
2022-01-05 13:02:36,038 epoch 4 - iter 852/2138 - loss 0.49798350 - samples/sec: 12.45 - lr: 0.000037
2022-01-05 13:03:11,958 epoch 4 - iter 1065/2138 - loss 0.48279803 - samples/sec: 11.99 - lr: 0.000036
2022-01-05 13:03:47,689 epoch 4 - iter 1278/2138 - loss 0.48210193 - samples/sec: 12.03 - lr: 0.000036
2022-01-05 13:04:23,014 epoch 4 - iter 1491/2138 - loss 0.48436311 - samples/sec: 12.14 - lr: 0.000035
2022-01-05 13:04:59,171 epoch 4 - iter 1704/2138 - loss 0.49445498 - samples/sec: 11.90 - lr: 0.000034
2022-01-05 13:05:33,735 epoch 4 - iter 1917/2138 - loss 0.49294978 - samples/sec: 12.44 - lr: 0.000034
2022-01-05 13:06:09,430 epoch 4 - iter 2130/2138 - loss 0.49137040 - samples/sec: 12.04 - lr: 0.000033
2022-01-05 13:06:10,977 ----------------------------------------------------------------------------------------------------
2022-01-05 13:06:10,977 EPOCH 4 done: loss 1048.4369 - lr 0.0000333
2022-01-05 13:06:31,872 DEV : loss 0.2979298532009125 - f1-score (micro avg)  0.8657
2022-01-05 13:06:33,304 BAD EPOCHS (no improvement): 4
2022-01-05 13:06:33,304 ----------------------------------------------------------------------------------------------------
2022-01-05 13:07:18,675 epoch 5 - iter 213/2138 - loss 0.38663209 - samples/sec: 12.28 - lr: 0.000033
2022-01-05 13:07:54,729 epoch 5 - iter 426/2138 - loss 0.37012698 - samples/sec: 11.93 - lr: 0.000032
2022-01-05 13:08:31,091 epoch 5 - iter 639/2138 - loss 0.36365512 - samples/sec: 11.82 - lr: 0.000032
2022-01-05 13:09:04,821 epoch 5 - iter 852/2138 - loss 0.35948537 - samples/sec: 12.71 - lr: 0.000031
2022-01-05 13:09:41,733 epoch 5 - iter 1065/2138 - loss 0.35597991 - samples/sec: 11.63 - lr: 0.000031
2022-01-05 13:10:17,152 epoch 5 - iter 1278/2138 - loss 0.35768831 - samples/sec: 12.13 - lr: 0.000030
2022-01-05 13:10:53,785 epoch 5 - iter 1491/2138 - loss 0.35968828 - samples/sec: 11.74 - lr: 0.000029
2022-01-05 13:11:29,376 epoch 5 - iter 1704/2138 - loss 0.35666324 - samples/sec: 12.09 - lr: 0.000029
2022-01-05 13:12:03,604 epoch 5 - iter 1917/2138 - loss 0.35068657 - samples/sec: 12.54 - lr: 0.000028
2022-01-05 13:12:37,467 epoch 5 - iter 2130/2138 - loss 0.34788347 - samples/sec: 12.71 - lr: 0.000028
2022-01-05 13:12:39,279 ----------------------------------------------------------------------------------------------------
2022-01-05 13:12:39,279 EPOCH 5 done: loss 742.2836 - lr 0.0000278
2022-01-05 13:12:59,831 DEV : loss 0.24587120115756989 - f1-score (micro avg)  0.8788
2022-01-05 13:13:01,247 BAD EPOCHS (no improvement): 4
2022-01-05 13:13:01,247 ----------------------------------------------------------------------------------------------------
2022-01-05 13:13:48,863 epoch 6 - iter 213/2138 - loss 0.19049994 - samples/sec: 11.53 - lr: 0.000027
2022-01-05 13:14:25,091 epoch 6 - iter 426/2138 - loss 0.21091442 - samples/sec: 11.84 - lr: 0.000027
2022-01-05 13:15:01,037 epoch 6 - iter 639/2138 - loss 0.23062014 - samples/sec: 11.95 - lr: 0.000026
2022-01-05 13:15:36,598 epoch 6 - iter 852/2138 - loss 0.23825005 - samples/sec: 12.09 - lr: 0.000026
2022-01-05 13:16:10,991 epoch 6 - iter 1065/2138 - loss 0.25425269 - samples/sec: 12.49 - lr: 0.000025
2022-01-05 13:16:45,106 epoch 6 - iter 1278/2138 - loss 0.24684050 - samples/sec: 12.56 - lr: 0.000024
2022-01-05 13:17:20,267 epoch 6 - iter 1491/2138 - loss 0.25187090 - samples/sec: 12.22 - lr: 0.000024
2022-01-05 13:17:55,786 epoch 6 - iter 1704/2138 - loss 0.24740880 - samples/sec: 12.09 - lr: 0.000023
2022-01-05 13:18:31,363 epoch 6 - iter 1917/2138 - loss 0.24473686 - samples/sec: 12.10 - lr: 0.000023
2022-01-05 13:19:05,645 epoch 6 - iter 2130/2138 - loss 0.24580396 - samples/sec: 12.51 - lr: 0.000022
2022-01-05 13:19:07,113 ----------------------------------------------------------------------------------------------------
2022-01-05 13:19:07,113 EPOCH 6 done: loss 525.1392 - lr 0.0000222
2022-01-05 13:19:27,684 DEV : loss 0.24098552763462067 - f1-score (micro avg)  0.8846
2022-01-05 13:19:29,226 BAD EPOCHS (no improvement): 4
2022-01-05 13:19:29,240 ----------------------------------------------------------------------------------------------------
2022-01-05 13:20:13,485 epoch 7 - iter 213/2138 - loss 0.17915050 - samples/sec: 12.68 - lr: 0.000022
2022-01-05 13:20:48,975 epoch 7 - iter 426/2138 - loss 0.17507239 - samples/sec: 12.12 - lr: 0.000021
2022-01-05 13:21:25,506 epoch 7 - iter 639/2138 - loss 0.17022061 - samples/sec: 11.77 - lr: 0.000021
2022-01-05 13:22:01,239 epoch 7 - iter 852/2138 - loss 0.16099910 - samples/sec: 12.03 - lr: 0.000020
2022-01-05 13:22:36,534 epoch 7 - iter 1065/2138 - loss 0.16285858 - samples/sec: 12.17 - lr: 0.000019
2022-01-05 13:23:11,273 epoch 7 - iter 1278/2138 - loss 0.16562312 - samples/sec: 12.39 - lr: 0.000019
2022-01-05 13:23:45,576 epoch 7 - iter 1491/2138 - loss 0.16814302 - samples/sec: 12.53 - lr: 0.000018
2022-01-05 13:24:20,738 epoch 7 - iter 1704/2138 - loss 0.16693049 - samples/sec: 12.28 - lr: 0.000018
2022-01-05 13:24:55,279 epoch 7 - iter 1917/2138 - loss 0.16427231 - samples/sec: 12.43 - lr: 0.000017
2022-01-05 13:25:30,370 epoch 7 - iter 2130/2138 - loss 0.16383677 - samples/sec: 12.23 - lr: 0.000017
2022-01-05 13:25:32,051 ----------------------------------------------------------------------------------------------------
2022-01-05 13:25:32,051 EPOCH 7 done: loss 350.0005 - lr 0.0000167
2022-01-05 13:25:52,665 DEV : loss 0.21952573955059052 - f1-score (micro avg)  0.8925
2022-01-05 13:25:54,086 BAD EPOCHS (no improvement): 4
2022-01-05 13:25:54,086 ----------------------------------------------------------------------------------------------------
2022-01-05 13:26:39,246 epoch 8 - iter 213/2138 - loss 0.10783851 - samples/sec: 12.40 - lr: 0.000016
2022-01-05 13:27:13,175 epoch 8 - iter 426/2138 - loss 0.11164049 - samples/sec: 12.67 - lr: 0.000016
2022-01-05 13:27:48,213 epoch 8 - iter 639/2138 - loss 0.11339409 - samples/sec: 12.26 - lr: 0.000015
2022-01-05 13:28:22,554 epoch 8 - iter 852/2138 - loss 0.11553554 - samples/sec: 12.52 - lr: 0.000014
2022-01-05 13:28:56,520 epoch 8 - iter 1065/2138 - loss 0.11388495 - samples/sec: 12.64 - lr: 0.000014
2022-01-05 13:29:30,770 epoch 8 - iter 1278/2138 - loss 0.11719912 - samples/sec: 12.54 - lr: 0.000013
2022-01-05 13:30:05,462 epoch 8 - iter 1491/2138 - loss 0.11511777 - samples/sec: 12.37 - lr: 0.000013
2022-01-05 13:30:40,580 epoch 8 - iter 1704/2138 - loss 0.11371319 - samples/sec: 12.26 - lr: 0.000012
2022-01-05 13:31:16,777 epoch 8 - iter 1917/2138 - loss 0.11574207 - samples/sec: 11.89 - lr: 0.000012
2022-01-05 13:31:53,038 epoch 8 - iter 2130/2138 - loss 0.11319059 - samples/sec: 11.87 - lr: 0.000011
2022-01-05 13:31:54,532 ----------------------------------------------------------------------------------------------------
2022-01-05 13:31:54,532 EPOCH 8 done: loss 243.3222 - lr 0.0000111
2022-01-05 13:32:15,078 DEV : loss 0.23308917880058289 - f1-score (micro avg)  0.8895
2022-01-05 13:32:16,478 BAD EPOCHS (no improvement): 4
2022-01-05 13:32:16,478 ----------------------------------------------------------------------------------------------------
2022-01-05 13:33:01,285 epoch 9 - iter 213/2138 - loss 0.07395980 - samples/sec: 12.51 - lr: 0.000011
2022-01-05 13:33:36,930 epoch 9 - iter 426/2138 - loss 0.08343041 - samples/sec: 12.07 - lr: 0.000010
2022-01-05 13:34:10,388 epoch 9 - iter 639/2138 - loss 0.08317172 - samples/sec: 12.86 - lr: 0.000009
2022-01-05 13:34:45,648 epoch 9 - iter 852/2138 - loss 0.08394120 - samples/sec: 12.19 - lr: 0.000009
2022-01-05 13:35:20,530 epoch 9 - iter 1065/2138 - loss 0.08384904 - samples/sec: 12.36 - lr: 0.000008
2022-01-05 13:35:53,383 epoch 9 - iter 1278/2138 - loss 0.08160532 - samples/sec: 13.04 - lr: 0.000008
2022-01-05 13:36:28,294 epoch 9 - iter 1491/2138 - loss 0.08057756 - samples/sec: 12.30 - lr: 0.000007
2022-01-05 13:37:04,481 epoch 9 - iter 1704/2138 - loss 0.08024387 - samples/sec: 11.88 - lr: 0.000007
2022-01-05 13:37:39,164 epoch 9 - iter 1917/2138 - loss 0.07919590 - samples/sec: 12.40 - lr: 0.000006
2022-01-05 13:38:13,937 epoch 9 - iter 2130/2138 - loss 0.07879799 - samples/sec: 12.38 - lr: 0.000006
2022-01-05 13:38:15,815 ----------------------------------------------------------------------------------------------------
2022-01-05 13:38:15,815 EPOCH 9 done: loss 168.7720 - lr 0.0000056
2022-01-05 13:38:36,422 DEV : loss 0.23830817639827728 - f1-score (micro avg)  0.887
2022-01-05 13:38:37,802 BAD EPOCHS (no improvement): 4
2022-01-05 13:38:37,802 ----------------------------------------------------------------------------------------------------
2022-01-05 13:39:22,508 epoch 10 - iter 213/2138 - loss 0.05129606 - samples/sec: 12.55 - lr: 0.000005
2022-01-05 13:39:57,786 epoch 10 - iter 426/2138 - loss 0.04695379 - samples/sec: 12.21 - lr: 0.000004
2022-01-05 13:40:33,275 epoch 10 - iter 639/2138 - loss 0.04805574 - samples/sec: 12.14 - lr: 0.000004
2022-01-05 13:41:08,166 epoch 10 - iter 852/2138 - loss 0.04623531 - samples/sec: 12.33 - lr: 0.000003
2022-01-05 13:41:41,452 epoch 10 - iter 1065/2138 - loss 0.05066929 - samples/sec: 12.93 - lr: 0.000003
2022-01-05 13:42:15,269 epoch 10 - iter 1278/2138 - loss 0.05104981 - samples/sec: 12.71 - lr: 0.000002
2022-01-05 13:42:50,671 epoch 10 - iter 1491/2138 - loss 0.05047000 - samples/sec: 12.15 - lr: 0.000002
2022-01-05 13:43:24,856 epoch 10 - iter 1704/2138 - loss 0.05178308 - samples/sec: 12.56 - lr: 0.000001
2022-01-05 13:44:00,614 epoch 10 - iter 1917/2138 - loss 0.05254779 - samples/sec: 12.00 - lr: 0.000001
2022-01-05 13:44:35,068 epoch 10 - iter 2130/2138 - loss 0.05274742 - samples/sec: 12.47 - lr: 0.000000
2022-01-05 13:44:36,318 ----------------------------------------------------------------------------------------------------
2022-01-05 13:44:36,318 EPOCH 10 done: loss 112.3898 - lr 0.0000000
2022-01-05 13:44:56,751 DEV : loss 0.2343454211950302 - f1-score (micro avg)  0.8904
2022-01-05 13:44:58,176 BAD EPOCHS (no improvement): 4
2022-01-05 13:44:59,086 ----------------------------------------------------------------------------------------------------
2022-01-05 13:44:59,086 Testing using last state of model ...
2022-01-05 13:45:14,151 0.5438115188467607	0.8655178239609231	0.872081125948772
2022-01-05 13:45:14,151 AVG: mse: 0.5438 - mae: 0.4979 - pearson: 0.8721 - spearman: 0.8655
2022-01-05 13:45:14,151 ----------------------------------------------------------------------------------------------------
