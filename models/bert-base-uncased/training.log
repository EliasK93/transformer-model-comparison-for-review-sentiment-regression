2022-01-05 11:02:07,987 ----------------------------------------------------------------------------------------------------
2022-01-05 11:02:07,989 Model: "TextRegressor(
  (document_embeddings): TransformerDocumentEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=1, bias=True)
  (loss_function): MSELoss()
)"
2022-01-05 11:02:08,017 ----------------------------------------------------------------------------------------------------
2022-01-05 11:02:08,017 Corpus: "Corpus: 4275 train + 475 dev + 250 test sentences"
2022-01-05 11:02:08,017 ----------------------------------------------------------------------------------------------------
2022-01-05 11:02:08,017 Parameters:
2022-01-05 11:02:08,017  - learning_rate: "5e-05"
2022-01-05 11:02:08,017  - mini_batch_size: "2"
2022-01-05 11:02:08,017  - patience: "3"
2022-01-05 11:02:08,017  - anneal_factor: "0.5"
2022-01-05 11:02:08,017  - max_epochs: "10"
2022-01-05 11:02:08,017  - shuffle: "True"
2022-01-05 11:02:08,017  - train_with_dev: "False"
2022-01-05 11:02:08,017  - batch_growth_annealing: "False"
2022-01-05 11:02:08,017 ----------------------------------------------------------------------------------------------------
2022-01-05 11:02:08,017 Model training base path: "models\bert-base-uncased_fine_tuned"
2022-01-05 11:02:08,017 ----------------------------------------------------------------------------------------------------
2022-01-05 11:02:08,017 Device: cuda:0
2022-01-05 11:02:08,017 ----------------------------------------------------------------------------------------------------
2022-01-05 11:02:08,017 Embeddings storage mode: gpu
2022-01-05 11:02:08,020 ----------------------------------------------------------------------------------------------------
2022-01-05 11:02:50,688 epoch 1 - iter 213/2138 - loss 4.07687438 - samples/sec: 12.78 - lr: 0.000005
2022-01-05 11:03:24,171 epoch 1 - iter 426/2138 - loss 2.62639448 - samples/sec: 12.83 - lr: 0.000010
2022-01-05 11:03:57,484 epoch 1 - iter 639/2138 - loss 2.07876015 - samples/sec: 12.92 - lr: 0.000015
2022-01-05 11:04:31,564 epoch 1 - iter 852/2138 - loss 1.78688799 - samples/sec: 12.61 - lr: 0.000020
2022-01-05 11:05:04,024 epoch 1 - iter 1065/2138 - loss 1.58059042 - samples/sec: 13.25 - lr: 0.000025
2022-01-05 11:05:37,538 epoch 1 - iter 1278/2138 - loss 1.46928216 - samples/sec: 12.84 - lr: 0.000030
2022-01-05 11:06:10,447 epoch 1 - iter 1491/2138 - loss 1.37911119 - samples/sec: 13.08 - lr: 0.000035
2022-01-05 11:06:44,971 epoch 1 - iter 1704/2138 - loss 1.31664220 - samples/sec: 12.46 - lr: 0.000040
2022-01-05 11:07:20,074 epoch 1 - iter 1917/2138 - loss 1.27578363 - samples/sec: 12.26 - lr: 0.000045
2022-01-05 11:07:55,239 epoch 1 - iter 2130/2138 - loss 1.24641357 - samples/sec: 12.25 - lr: 0.000050
2022-01-05 11:07:56,973 ----------------------------------------------------------------------------------------------------
2022-01-05 11:07:56,973 EPOCH 1 done: loss 2662.4715 - lr 0.0000500
2022-01-05 11:08:17,452 DEV : loss 0.4204331338405609 - f1-score (micro avg)  0.8215
2022-01-05 11:08:18,924 BAD EPOCHS (no improvement): 4
2022-01-05 11:08:18,924 ----------------------------------------------------------------------------------------------------
2022-01-05 11:09:03,180 epoch 2 - iter 213/2138 - loss 0.82301366 - samples/sec: 12.68 - lr: 0.000049
2022-01-05 11:09:36,780 epoch 2 - iter 426/2138 - loss 0.76528833 - samples/sec: 12.80 - lr: 0.000049
2022-01-05 11:10:11,089 epoch 2 - iter 639/2138 - loss 0.72289206 - samples/sec: 12.54 - lr: 0.000048
2022-01-05 11:10:46,419 epoch 2 - iter 852/2138 - loss 0.71924660 - samples/sec: 12.15 - lr: 0.000048
2022-01-05 11:11:21,328 epoch 2 - iter 1065/2138 - loss 0.71211223 - samples/sec: 12.31 - lr: 0.000047
2022-01-05 11:11:55,997 epoch 2 - iter 1278/2138 - loss 0.69750121 - samples/sec: 12.39 - lr: 0.000047
2022-01-05 11:12:31,248 epoch 2 - iter 1491/2138 - loss 0.68703350 - samples/sec: 12.19 - lr: 0.000046
2022-01-05 11:13:06,137 epoch 2 - iter 1704/2138 - loss 0.67840493 - samples/sec: 12.34 - lr: 0.000046
2022-01-05 11:13:39,567 epoch 2 - iter 1917/2138 - loss 0.67141143 - samples/sec: 12.87 - lr: 0.000045
2022-01-05 11:14:13,982 epoch 2 - iter 2130/2138 - loss 0.66169574 - samples/sec: 12.45 - lr: 0.000044
2022-01-05 11:14:15,507 ----------------------------------------------------------------------------------------------------
2022-01-05 11:14:15,507 EPOCH 2 done: loss 1414.5279 - lr 0.0000444
2022-01-05 11:14:36,233 DEV : loss 0.28213968873023987 - f1-score (micro avg)  0.8643
2022-01-05 11:14:37,715 BAD EPOCHS (no improvement): 4
2022-01-05 11:14:37,715 ----------------------------------------------------------------------------------------------------
2022-01-05 11:15:21,311 epoch 3 - iter 213/2138 - loss 0.38604270 - samples/sec: 13.04 - lr: 0.000044
2022-01-05 11:15:56,853 epoch 3 - iter 426/2138 - loss 0.40056796 - samples/sec: 12.11 - lr: 0.000043
2022-01-05 11:16:32,249 epoch 3 - iter 639/2138 - loss 0.42558283 - samples/sec: 12.14 - lr: 0.000043
2022-01-05 11:17:06,353 epoch 3 - iter 852/2138 - loss 0.42433715 - samples/sec: 12.59 - lr: 0.000042
2022-01-05 11:17:40,088 epoch 3 - iter 1065/2138 - loss 0.42270486 - samples/sec: 12.75 - lr: 0.000042
2022-01-05 11:18:13,816 epoch 3 - iter 1278/2138 - loss 0.41883675 - samples/sec: 12.77 - lr: 0.000041
2022-01-05 11:18:49,366 epoch 3 - iter 1491/2138 - loss 0.41397425 - samples/sec: 12.09 - lr: 0.000041
2022-01-05 11:19:22,034 epoch 3 - iter 1704/2138 - loss 0.41741146 - samples/sec: 13.16 - lr: 0.000040
2022-01-05 11:19:56,509 epoch 3 - iter 1917/2138 - loss 0.40773429 - samples/sec: 12.51 - lr: 0.000039
2022-01-05 11:20:31,577 epoch 3 - iter 2130/2138 - loss 0.40961482 - samples/sec: 12.28 - lr: 0.000039
2022-01-05 11:20:32,962 ----------------------------------------------------------------------------------------------------
2022-01-05 11:20:32,962 EPOCH 3 done: loss 879.5212 - lr 0.0000389
2022-01-05 11:20:53,631 DEV : loss 0.2572092115879059 - f1-score (micro avg)  0.8715
2022-01-05 11:20:55,081 BAD EPOCHS (no improvement): 4
2022-01-05 11:20:55,081 ----------------------------------------------------------------------------------------------------
2022-01-05 11:21:38,998 epoch 4 - iter 213/2138 - loss 0.29589618 - samples/sec: 12.89 - lr: 0.000038
2022-01-05 11:22:15,676 epoch 4 - iter 426/2138 - loss 0.28312058 - samples/sec: 11.75 - lr: 0.000038
2022-01-05 11:22:48,168 epoch 4 - iter 639/2138 - loss 0.27679356 - samples/sec: 13.25 - lr: 0.000037
2022-01-05 11:23:23,307 epoch 4 - iter 852/2138 - loss 0.26931579 - samples/sec: 12.24 - lr: 0.000037
2022-01-05 11:23:57,116 epoch 4 - iter 1065/2138 - loss 0.27119039 - samples/sec: 12.71 - lr: 0.000036
2022-01-05 11:24:30,536 epoch 4 - iter 1278/2138 - loss 0.26715088 - samples/sec: 12.89 - lr: 0.000036
2022-01-05 11:25:05,646 epoch 4 - iter 1491/2138 - loss 0.27120124 - samples/sec: 12.22 - lr: 0.000035
2022-01-05 11:25:39,417 epoch 4 - iter 1704/2138 - loss 0.27048179 - samples/sec: 12.72 - lr: 0.000034
2022-01-05 11:26:13,691 epoch 4 - iter 1917/2138 - loss 0.26541187 - samples/sec: 12.52 - lr: 0.000034
2022-01-05 11:26:48,863 epoch 4 - iter 2130/2138 - loss 0.26241029 - samples/sec: 12.22 - lr: 0.000033
2022-01-05 11:26:50,648 ----------------------------------------------------------------------------------------------------
2022-01-05 11:26:50,648 EPOCH 4 done: loss 560.3002 - lr 0.0000333
2022-01-05 11:27:11,317 DEV : loss 0.29311931133270264 - f1-score (micro avg)  0.8768
2022-01-05 11:27:12,817 BAD EPOCHS (no improvement): 4
2022-01-05 11:27:12,817 ----------------------------------------------------------------------------------------------------
2022-01-05 11:27:57,673 epoch 5 - iter 213/2138 - loss 0.19830924 - samples/sec: 12.43 - lr: 0.000033
2022-01-05 11:28:31,131 epoch 5 - iter 426/2138 - loss 0.18941154 - samples/sec: 12.86 - lr: 0.000032
2022-01-05 11:29:04,891 epoch 5 - iter 639/2138 - loss 0.19473059 - samples/sec: 12.72 - lr: 0.000032
2022-01-05 11:29:37,702 epoch 5 - iter 852/2138 - loss 0.18966368 - samples/sec: 13.07 - lr: 0.000031
2022-01-05 11:30:12,034 epoch 5 - iter 1065/2138 - loss 0.18741428 - samples/sec: 12.53 - lr: 0.000031
2022-01-05 11:30:45,903 epoch 5 - iter 1278/2138 - loss 0.18207716 - samples/sec: 12.74 - lr: 0.000030
2022-01-05 11:31:21,825 epoch 5 - iter 1491/2138 - loss 0.17854053 - samples/sec: 11.97 - lr: 0.000029
2022-01-05 11:31:56,948 epoch 5 - iter 1704/2138 - loss 0.17795010 - samples/sec: 12.28 - lr: 0.000029
2022-01-05 11:32:31,476 epoch 5 - iter 1917/2138 - loss 0.17642112 - samples/sec: 12.46 - lr: 0.000028
2022-01-05 11:33:05,567 epoch 5 - iter 2130/2138 - loss 0.17367184 - samples/sec: 12.64 - lr: 0.000028
2022-01-05 11:33:07,179 ----------------------------------------------------------------------------------------------------
2022-01-05 11:33:07,179 EPOCH 5 done: loss 370.7552 - lr 0.0000278
2022-01-05 11:33:27,842 DEV : loss 0.21575480699539185 - f1-score (micro avg)  0.8944
2022-01-05 11:33:29,252 BAD EPOCHS (no improvement): 4
2022-01-05 11:33:29,252 ----------------------------------------------------------------------------------------------------
2022-01-05 11:34:13,660 epoch 6 - iter 213/2138 - loss 0.12317269 - samples/sec: 12.65 - lr: 0.000027
2022-01-05 11:34:47,371 epoch 6 - iter 426/2138 - loss 0.12039406 - samples/sec: 12.74 - lr: 0.000027
2022-01-05 11:35:21,235 epoch 6 - iter 639/2138 - loss 0.11203668 - samples/sec: 12.69 - lr: 0.000026
2022-01-05 11:35:56,378 epoch 6 - iter 852/2138 - loss 0.11545209 - samples/sec: 12.24 - lr: 0.000026
2022-01-05 11:36:31,438 epoch 6 - iter 1065/2138 - loss 0.11197496 - samples/sec: 12.27 - lr: 0.000025
2022-01-05 11:37:05,710 epoch 6 - iter 1278/2138 - loss 0.11210010 - samples/sec: 12.52 - lr: 0.000024
2022-01-05 11:37:39,356 epoch 6 - iter 1491/2138 - loss 0.11476706 - samples/sec: 12.77 - lr: 0.000024
2022-01-05 11:38:14,100 epoch 6 - iter 1704/2138 - loss 0.11444367 - samples/sec: 12.37 - lr: 0.000023
2022-01-05 11:38:48,542 epoch 6 - iter 1917/2138 - loss 0.11342417 - samples/sec: 12.46 - lr: 0.000023
2022-01-05 11:39:21,240 epoch 6 - iter 2130/2138 - loss 0.11073356 - samples/sec: 13.14 - lr: 0.000022
2022-01-05 11:39:22,853 ----------------------------------------------------------------------------------------------------
2022-01-05 11:39:22,853 EPOCH 6 done: loss 237.4981 - lr 0.0000222
2022-01-05 11:39:43,541 DEV : loss 0.1922321319580078 - f1-score (micro avg)  0.9037
2022-01-05 11:39:44,966 BAD EPOCHS (no improvement): 4
2022-01-05 11:39:44,982 ----------------------------------------------------------------------------------------------------
2022-01-05 11:40:29,319 epoch 7 - iter 213/2138 - loss 0.07469438 - samples/sec: 12.67 - lr: 0.000022
2022-01-05 11:41:03,235 epoch 7 - iter 426/2138 - loss 0.06883623 - samples/sec: 12.68 - lr: 0.000021
2022-01-05 11:41:36,135 epoch 7 - iter 639/2138 - loss 0.07189728 - samples/sec: 13.04 - lr: 0.000021
2022-01-05 11:42:10,702 epoch 7 - iter 852/2138 - loss 0.06937329 - samples/sec: 12.45 - lr: 0.000020
2022-01-05 11:42:42,525 epoch 7 - iter 1065/2138 - loss 0.06692979 - samples/sec: 13.52 - lr: 0.000019
2022-01-05 11:43:16,152 epoch 7 - iter 1278/2138 - loss 0.06761426 - samples/sec: 12.80 - lr: 0.000019
2022-01-05 11:43:50,007 epoch 7 - iter 1491/2138 - loss 0.06862996 - samples/sec: 12.63 - lr: 0.000018
2022-01-05 11:44:24,613 epoch 7 - iter 1704/2138 - loss 0.06679181 - samples/sec: 12.45 - lr: 0.000018
2022-01-05 11:44:58,420 epoch 7 - iter 1917/2138 - loss 0.06663455 - samples/sec: 12.69 - lr: 0.000017
2022-01-05 11:45:32,591 epoch 7 - iter 2130/2138 - loss 0.06537355 - samples/sec: 12.58 - lr: 0.000017
2022-01-05 11:45:34,007 ----------------------------------------------------------------------------------------------------
2022-01-05 11:45:34,007 EPOCH 7 done: loss 139.3175 - lr 0.0000167
2022-01-05 11:45:54,718 DEV : loss 0.20826376974582672 - f1-score (micro avg)  0.8998
2022-01-05 11:45:56,181 BAD EPOCHS (no improvement): 4
2022-01-05 11:45:56,181 ----------------------------------------------------------------------------------------------------
2022-01-05 11:46:39,952 epoch 8 - iter 213/2138 - loss 0.03972568 - samples/sec: 12.92 - lr: 0.000016
2022-01-05 11:47:13,278 epoch 8 - iter 426/2138 - loss 0.04015020 - samples/sec: 12.94 - lr: 0.000016
2022-01-05 11:47:46,845 epoch 8 - iter 639/2138 - loss 0.04106852 - samples/sec: 12.80 - lr: 0.000015
2022-01-05 11:48:19,754 epoch 8 - iter 852/2138 - loss 0.04015467 - samples/sec: 13.08 - lr: 0.000014
2022-01-05 11:48:54,293 epoch 8 - iter 1065/2138 - loss 0.03772359 - samples/sec: 12.45 - lr: 0.000014
2022-01-05 11:49:28,218 epoch 8 - iter 1278/2138 - loss 0.03517220 - samples/sec: 12.72 - lr: 0.000013
2022-01-05 11:50:01,408 epoch 8 - iter 1491/2138 - loss 0.03382791 - samples/sec: 12.96 - lr: 0.000013
2022-01-05 11:50:34,260 epoch 8 - iter 1704/2138 - loss 0.03275577 - samples/sec: 13.07 - lr: 0.000012
2022-01-05 11:51:07,845 epoch 8 - iter 1917/2138 - loss 0.03313855 - samples/sec: 12.79 - lr: 0.000012
2022-01-05 11:51:41,923 epoch 8 - iter 2130/2138 - loss 0.03199164 - samples/sec: 12.60 - lr: 0.000011
2022-01-05 11:51:43,276 ----------------------------------------------------------------------------------------------------
2022-01-05 11:51:43,276 EPOCH 8 done: loss 68.1555 - lr 0.0000111
2022-01-05 11:52:03,954 DEV : loss 0.20392194390296936 - f1-score (micro avg)  0.8997
2022-01-05 11:52:05,401 BAD EPOCHS (no improvement): 4
2022-01-05 11:52:05,401 ----------------------------------------------------------------------------------------------------
2022-01-05 11:52:48,635 epoch 9 - iter 213/2138 - loss 0.02221675 - samples/sec: 13.09 - lr: 0.000011
2022-01-05 11:53:22,152 epoch 9 - iter 426/2138 - loss 0.01402621 - samples/sec: 12.84 - lr: 0.000010
2022-01-05 11:53:54,869 epoch 9 - iter 639/2138 - loss 0.01433187 - samples/sec: 13.14 - lr: 0.000009
2022-01-05 11:54:28,257 epoch 9 - iter 852/2138 - loss 0.01487081 - samples/sec: 12.89 - lr: 0.000009
2022-01-05 11:55:02,527 epoch 9 - iter 1065/2138 - loss 0.01376094 - samples/sec: 12.52 - lr: 0.000008
2022-01-05 11:55:35,706 epoch 9 - iter 1278/2138 - loss 0.01481253 - samples/sec: 12.99 - lr: 0.000008
2022-01-05 11:56:09,822 epoch 9 - iter 1491/2138 - loss 0.01402745 - samples/sec: 12.62 - lr: 0.000007
2022-01-05 11:56:44,442 epoch 9 - iter 1704/2138 - loss 0.01398788 - samples/sec: 12.43 - lr: 0.000007
2022-01-05 11:57:16,986 epoch 9 - iter 1917/2138 - loss 0.01561112 - samples/sec: 13.23 - lr: 0.000006
2022-01-05 11:57:49,803 epoch 9 - iter 2130/2138 - loss 0.01589269 - samples/sec: 13.12 - lr: 0.000006
2022-01-05 11:57:51,234 ----------------------------------------------------------------------------------------------------
2022-01-05 11:57:51,234 EPOCH 9 done: loss 34.3673 - lr 0.0000056
2022-01-05 11:58:11,836 DEV : loss 0.19869039952754974 - f1-score (micro avg)  0.901
2022-01-05 11:58:13,257 BAD EPOCHS (no improvement): 4
2022-01-05 11:58:13,257 ----------------------------------------------------------------------------------------------------
2022-01-05 11:58:56,307 epoch 10 - iter 213/2138 - loss 0.00732761 - samples/sec: 13.17 - lr: 0.000005
2022-01-05 11:59:29,964 epoch 10 - iter 426/2138 - loss 0.01233473 - samples/sec: 12.81 - lr: 0.000004
2022-01-05 12:00:02,894 epoch 10 - iter 639/2138 - loss 0.00930393 - samples/sec: 13.06 - lr: 0.000004
2022-01-05 12:00:37,283 epoch 10 - iter 852/2138 - loss 0.00905582 - samples/sec: 12.51 - lr: 0.000003
2022-01-05 12:01:11,524 epoch 10 - iter 1065/2138 - loss 0.00931029 - samples/sec: 12.58 - lr: 0.000003
2022-01-05 12:01:45,728 epoch 10 - iter 1278/2138 - loss 0.00869496 - samples/sec: 12.54 - lr: 0.000002
2022-01-05 12:02:19,043 epoch 10 - iter 1491/2138 - loss 0.00842487 - samples/sec: 12.88 - lr: 0.000002
2022-01-05 12:02:52,745 epoch 10 - iter 1704/2138 - loss 0.00868236 - samples/sec: 12.79 - lr: 0.000001
2022-01-05 12:03:25,476 epoch 10 - iter 1917/2138 - loss 0.00814219 - samples/sec: 13.15 - lr: 0.000001
2022-01-05 12:03:58,584 epoch 10 - iter 2130/2138 - loss 0.00825243 - samples/sec: 12.99 - lr: 0.000000
2022-01-05 12:04:00,193 ----------------------------------------------------------------------------------------------------
2022-01-05 12:04:00,193 EPOCH 10 done: loss 17.5870 - lr 0.0000000
2022-01-05 12:04:20,936 DEV : loss 0.20682352781295776 - f1-score (micro avg)  0.8997
2022-01-05 12:04:22,337 BAD EPOCHS (no improvement): 4
2022-01-05 12:04:23,123 ----------------------------------------------------------------------------------------------------
2022-01-05 12:04:23,138 Testing using last state of model ...
2022-01-05 12:04:38,282 0.40123076321273915	0.8992663608980377	0.9003480406082105
2022-01-05 12:04:38,282 AVG: mse: 0.4012 - mae: 0.3706 - pearson: 0.9003 - spearman: 0.8993
2022-01-05 12:04:38,282 ----------------------------------------------------------------------------------------------------
