2022-01-05 23:10:42,311 ----------------------------------------------------------------------------------------------------
2022-01-05 23:10:42,312 Model: "TextRegressor(
  (document_embeddings): TransformerDocumentEmbeddings(
    (model): DistilBertModel(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer): Transformer(
        (layer): ModuleList(
          (0): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (1): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (2): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (3): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (4): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
          (5): TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=1, bias=True)
  (loss_function): MSELoss()
)"
2022-01-05 23:10:42,325 ----------------------------------------------------------------------------------------------------
2022-01-05 23:10:42,325 Corpus: "Corpus: 4275 train + 475 dev + 250 test sentences"
2022-01-05 23:10:42,325 ----------------------------------------------------------------------------------------------------
2022-01-05 23:10:42,325 Parameters:
2022-01-05 23:10:42,325  - learning_rate: "5e-05"
2022-01-05 23:10:42,325  - mini_batch_size: "2"
2022-01-05 23:10:42,325  - patience: "3"
2022-01-05 23:10:42,325  - anneal_factor: "0.5"
2022-01-05 23:10:42,325  - max_epochs: "10"
2022-01-05 23:10:42,325  - shuffle: "True"
2022-01-05 23:10:42,325  - train_with_dev: "False"
2022-01-05 23:10:42,326  - batch_growth_annealing: "False"
2022-01-05 23:10:42,326 ----------------------------------------------------------------------------------------------------
2022-01-05 23:10:42,326 Model training base path: "models\distilbert-base-cased"
2022-01-05 23:10:42,326 ----------------------------------------------------------------------------------------------------
2022-01-05 23:10:42,326 Device: cuda:0
2022-01-05 23:10:42,326 ----------------------------------------------------------------------------------------------------
2022-01-05 23:10:42,326 Embeddings storage mode: gpu
2022-01-05 23:10:42,328 ----------------------------------------------------------------------------------------------------
2022-01-05 23:11:12,278 epoch 1 - iter 213/2138 - loss 3.92233298 - samples/sec: 22.07 - lr: 0.000005
2022-01-05 23:11:32,198 epoch 1 - iter 426/2138 - loss 2.59228997 - samples/sec: 21.72 - lr: 0.000010
2022-01-05 23:11:52,118 epoch 1 - iter 639/2138 - loss 2.09205282 - samples/sec: 21.74 - lr: 0.000015
2022-01-05 23:12:12,322 epoch 1 - iter 852/2138 - loss 1.84697670 - samples/sec: 21.39 - lr: 0.000020
2022-01-05 23:12:32,212 epoch 1 - iter 1065/2138 - loss 1.67314787 - samples/sec: 21.72 - lr: 0.000025
2022-01-05 23:12:52,526 epoch 1 - iter 1278/2138 - loss 1.57224376 - samples/sec: 21.32 - lr: 0.000030
2022-01-05 23:13:12,246 epoch 1 - iter 1491/2138 - loss 1.48808051 - samples/sec: 21.91 - lr: 0.000035
2022-01-05 23:13:32,405 epoch 1 - iter 1704/2138 - loss 1.42631215 - samples/sec: 21.45 - lr: 0.000040
2022-01-05 23:13:53,093 epoch 1 - iter 1917/2138 - loss 1.39984697 - samples/sec: 20.98 - lr: 0.000045
2022-01-05 23:14:13,944 epoch 1 - iter 2130/2138 - loss 1.35865586 - samples/sec: 20.76 - lr: 0.000050
2022-01-05 23:14:15,119 ----------------------------------------------------------------------------------------------------
2022-01-05 23:14:15,119 EPOCH 1 done: loss 2904.5751 - lr 0.0000500
2022-01-05 23:14:32,292 DEV : loss 0.5537533760070801 - f1-score (micro avg)  0.7987
2022-01-05 23:14:33,799 BAD EPOCHS (no improvement): 4
2022-01-05 23:14:33,799 ----------------------------------------------------------------------------------------------------
2022-01-05 23:15:04,210 epoch 2 - iter 213/2138 - loss 0.89395004 - samples/sec: 22.00 - lr: 0.000049
2022-01-05 23:15:23,572 epoch 2 - iter 426/2138 - loss 0.81933336 - samples/sec: 22.30 - lr: 0.000049
2022-01-05 23:15:43,539 epoch 2 - iter 639/2138 - loss 0.78400853 - samples/sec: 21.73 - lr: 0.000048
2022-01-05 23:16:02,019 epoch 2 - iter 852/2138 - loss 0.78195670 - samples/sec: 23.35 - lr: 0.000048
2022-01-05 23:16:22,979 epoch 2 - iter 1065/2138 - loss 0.78452033 - samples/sec: 20.70 - lr: 0.000047
2022-01-05 23:16:43,109 epoch 2 - iter 1278/2138 - loss 0.77122162 - samples/sec: 21.49 - lr: 0.000047
2022-01-05 23:17:03,079 epoch 2 - iter 1491/2138 - loss 0.76206804 - samples/sec: 21.67 - lr: 0.000046
2022-01-05 23:17:24,228 epoch 2 - iter 1704/2138 - loss 0.75005407 - samples/sec: 20.48 - lr: 0.000046
2022-01-05 23:17:45,704 epoch 2 - iter 1917/2138 - loss 0.74385540 - samples/sec: 20.14 - lr: 0.000045
2022-01-05 23:18:05,439 epoch 2 - iter 2130/2138 - loss 0.74014886 - samples/sec: 21.93 - lr: 0.000044
2022-01-05 23:18:06,390 ----------------------------------------------------------------------------------------------------
2022-01-05 23:18:06,390 EPOCH 2 done: loss 1581.2896 - lr 0.0000444
2022-01-05 23:18:23,496 DEV : loss 0.31245556473731995 - f1-score (micro avg)  0.8463
2022-01-05 23:18:25,046 BAD EPOCHS (no improvement): 4
2022-01-05 23:18:25,046 ----------------------------------------------------------------------------------------------------
2022-01-05 23:18:54,737 epoch 3 - iter 213/2138 - loss 0.38922813 - samples/sec: 22.93 - lr: 0.000044
2022-01-05 23:19:15,109 epoch 3 - iter 426/2138 - loss 0.43743826 - samples/sec: 21.24 - lr: 0.000043
2022-01-05 23:19:34,741 epoch 3 - iter 639/2138 - loss 0.43852873 - samples/sec: 22.04 - lr: 0.000043
2022-01-05 23:19:54,330 epoch 3 - iter 852/2138 - loss 0.45298383 - samples/sec: 22.10 - lr: 0.000042
2022-01-05 23:20:14,393 epoch 3 - iter 1065/2138 - loss 0.43726288 - samples/sec: 21.57 - lr: 0.000042
2022-01-05 23:20:34,496 epoch 3 - iter 1278/2138 - loss 0.43057827 - samples/sec: 21.55 - lr: 0.000041
2022-01-05 23:20:55,037 epoch 3 - iter 1491/2138 - loss 0.43438238 - samples/sec: 21.07 - lr: 0.000041
2022-01-05 23:21:16,199 epoch 3 - iter 1704/2138 - loss 0.44520272 - samples/sec: 20.43 - lr: 0.000040
2022-01-05 23:21:37,541 epoch 3 - iter 1917/2138 - loss 0.44517089 - samples/sec: 20.29 - lr: 0.000039
2022-01-05 23:21:57,821 epoch 3 - iter 2130/2138 - loss 0.43978814 - samples/sec: 21.34 - lr: 0.000039
2022-01-05 23:21:58,767 ----------------------------------------------------------------------------------------------------
2022-01-05 23:21:58,767 EPOCH 3 done: loss 940.2757 - lr 0.0000389
2022-01-05 23:22:15,659 DEV : loss 0.31177595257759094 - f1-score (micro avg)  0.8635
2022-01-05 23:22:17,110 BAD EPOCHS (no improvement): 4
2022-01-05 23:22:17,110 ----------------------------------------------------------------------------------------------------
2022-01-05 23:22:48,484 epoch 4 - iter 213/2138 - loss 0.30269317 - samples/sec: 20.87 - lr: 0.000038
2022-01-05 23:23:08,145 epoch 4 - iter 426/2138 - loss 0.30880823 - samples/sec: 22.02 - lr: 0.000038
2022-01-05 23:23:28,319 epoch 4 - iter 639/2138 - loss 0.29457530 - samples/sec: 21.46 - lr: 0.000037
2022-01-05 23:23:48,674 epoch 4 - iter 852/2138 - loss 0.28574438 - samples/sec: 21.31 - lr: 0.000037
2022-01-05 23:24:09,245 epoch 4 - iter 1065/2138 - loss 0.29090507 - samples/sec: 21.04 - lr: 0.000036
2022-01-05 23:24:28,833 epoch 4 - iter 1278/2138 - loss 0.28766245 - samples/sec: 22.03 - lr: 0.000036
2022-01-05 23:24:50,480 epoch 4 - iter 1491/2138 - loss 0.29200113 - samples/sec: 20.00 - lr: 0.000035
2022-01-05 23:25:12,002 epoch 4 - iter 1704/2138 - loss 0.28742492 - samples/sec: 20.10 - lr: 0.000034
2022-01-05 23:25:32,942 epoch 4 - iter 1917/2138 - loss 0.28643083 - samples/sec: 20.66 - lr: 0.000034
2022-01-05 23:25:52,628 epoch 4 - iter 2130/2138 - loss 0.28835377 - samples/sec: 21.91 - lr: 0.000033
2022-01-05 23:25:53,617 ----------------------------------------------------------------------------------------------------
2022-01-05 23:25:53,618 EPOCH 4 done: loss 616.9738 - lr 0.0000333
2022-01-05 23:26:10,447 DEV : loss 0.24009035527706146 - f1-score (micro avg)  0.8827
2022-01-05 23:26:11,859 BAD EPOCHS (no improvement): 4
2022-01-05 23:26:11,859 ----------------------------------------------------------------------------------------------------
2022-01-05 23:26:40,852 epoch 5 - iter 213/2138 - loss 0.16234377 - samples/sec: 23.46 - lr: 0.000033
2022-01-05 23:27:01,096 epoch 5 - iter 426/2138 - loss 0.17668033 - samples/sec: 21.40 - lr: 0.000032
2022-01-05 23:27:20,865 epoch 5 - iter 639/2138 - loss 0.17019221 - samples/sec: 21.83 - lr: 0.000032
2022-01-05 23:27:40,812 epoch 5 - iter 852/2138 - loss 0.17327070 - samples/sec: 21.70 - lr: 0.000031
2022-01-05 23:28:00,348 epoch 5 - iter 1065/2138 - loss 0.17760565 - samples/sec: 22.15 - lr: 0.000031
2022-01-05 23:28:20,637 epoch 5 - iter 1278/2138 - loss 0.17502808 - samples/sec: 21.32 - lr: 0.000030
2022-01-05 23:28:39,724 epoch 5 - iter 1491/2138 - loss 0.17716203 - samples/sec: 22.67 - lr: 0.000029
2022-01-05 23:28:59,757 epoch 5 - iter 1704/2138 - loss 0.17758479 - samples/sec: 21.60 - lr: 0.000029
2022-01-05 23:29:20,187 epoch 5 - iter 1917/2138 - loss 0.17788043 - samples/sec: 21.16 - lr: 0.000028
2022-01-05 23:29:40,553 epoch 5 - iter 2130/2138 - loss 0.17560636 - samples/sec: 21.24 - lr: 0.000028
2022-01-05 23:29:41,716 ----------------------------------------------------------------------------------------------------
2022-01-05 23:29:41,716 EPOCH 5 done: loss 374.8575 - lr 0.0000278
2022-01-05 23:29:58,507 DEV : loss 0.28756093978881836 - f1-score (micro avg)  0.8751
2022-01-05 23:29:59,980 BAD EPOCHS (no improvement): 4
2022-01-05 23:29:59,980 ----------------------------------------------------------------------------------------------------
2022-01-05 23:30:30,101 epoch 6 - iter 213/2138 - loss 0.14042288 - samples/sec: 22.16 - lr: 0.000027
2022-01-05 23:30:49,771 epoch 6 - iter 426/2138 - loss 0.11997031 - samples/sec: 22.00 - lr: 0.000027
2022-01-05 23:31:09,488 epoch 6 - iter 639/2138 - loss 0.11727928 - samples/sec: 21.92 - lr: 0.000026
2022-01-05 23:31:29,452 epoch 6 - iter 852/2138 - loss 0.11544030 - samples/sec: 21.70 - lr: 0.000026
2022-01-05 23:31:50,421 epoch 6 - iter 1065/2138 - loss 0.11811322 - samples/sec: 20.63 - lr: 0.000025
2022-01-05 23:32:11,372 epoch 6 - iter 1278/2138 - loss 0.11813978 - samples/sec: 20.70 - lr: 0.000024
2022-01-05 23:32:31,010 epoch 6 - iter 1491/2138 - loss 0.11911962 - samples/sec: 22.02 - lr: 0.000024
2022-01-05 23:32:50,927 epoch 6 - iter 1704/2138 - loss 0.11962289 - samples/sec: 21.75 - lr: 0.000023
2022-01-05 23:33:10,731 epoch 6 - iter 1917/2138 - loss 0.11918978 - samples/sec: 21.84 - lr: 0.000023
2022-01-05 23:33:30,536 epoch 6 - iter 2130/2138 - loss 0.11770828 - samples/sec: 21.81 - lr: 0.000022
2022-01-05 23:33:31,747 ----------------------------------------------------------------------------------------------------
2022-01-05 23:33:31,747 EPOCH 6 done: loss 251.2410 - lr 0.0000222
2022-01-05 23:33:48,973 DEV : loss 0.21395356953144073 - f1-score (micro avg)  0.8948
2022-01-05 23:33:50,446 BAD EPOCHS (no improvement): 4
2022-01-05 23:33:50,484 ----------------------------------------------------------------------------------------------------
2022-01-05 23:34:20,599 epoch 7 - iter 213/2138 - loss 0.08942638 - samples/sec: 22.52 - lr: 0.000022
2022-01-05 23:34:40,055 epoch 7 - iter 426/2138 - loss 0.08698637 - samples/sec: 22.25 - lr: 0.000021
2022-01-05 23:34:59,380 epoch 7 - iter 639/2138 - loss 0.07879661 - samples/sec: 22.35 - lr: 0.000021
2022-01-05 23:35:19,964 epoch 7 - iter 852/2138 - loss 0.07605715 - samples/sec: 21.08 - lr: 0.000020
2022-01-05 23:35:39,424 epoch 7 - iter 1065/2138 - loss 0.07619825 - samples/sec: 22.19 - lr: 0.000019
2022-01-05 23:35:59,004 epoch 7 - iter 1278/2138 - loss 0.07403401 - samples/sec: 22.07 - lr: 0.000019
2022-01-05 23:36:19,156 epoch 7 - iter 1491/2138 - loss 0.07262329 - samples/sec: 21.43 - lr: 0.000018
2022-01-05 23:36:38,487 epoch 7 - iter 1704/2138 - loss 0.06974042 - samples/sec: 22.31 - lr: 0.000018
2022-01-05 23:36:59,328 epoch 7 - iter 1917/2138 - loss 0.06804322 - samples/sec: 20.80 - lr: 0.000017
2022-01-05 23:37:18,999 epoch 7 - iter 2130/2138 - loss 0.06752882 - samples/sec: 21.91 - lr: 0.000017
2022-01-05 23:37:20,149 ----------------------------------------------------------------------------------------------------
2022-01-05 23:37:20,149 EPOCH 7 done: loss 144.5605 - lr 0.0000167
2022-01-05 23:37:37,173 DEV : loss 0.22300441563129425 - f1-score (micro avg)  0.8914
2022-01-05 23:37:38,679 BAD EPOCHS (no improvement): 4
2022-01-05 23:37:38,679 ----------------------------------------------------------------------------------------------------
2022-01-05 23:38:08,791 epoch 8 - iter 213/2138 - loss 0.03700743 - samples/sec: 22.43 - lr: 0.000016
2022-01-05 23:38:28,463 epoch 8 - iter 426/2138 - loss 0.03209007 - samples/sec: 22.00 - lr: 0.000016
2022-01-05 23:38:48,299 epoch 8 - iter 639/2138 - loss 0.03456341 - samples/sec: 21.85 - lr: 0.000015
2022-01-05 23:39:07,954 epoch 8 - iter 852/2138 - loss 0.03237877 - samples/sec: 22.10 - lr: 0.000014
2022-01-05 23:39:28,051 epoch 8 - iter 1065/2138 - loss 0.03487659 - samples/sec: 21.51 - lr: 0.000014
2022-01-05 23:39:47,000 epoch 8 - iter 1278/2138 - loss 0.03539289 - samples/sec: 22.83 - lr: 0.000013
2022-01-05 23:40:07,139 epoch 8 - iter 1491/2138 - loss 0.03795600 - samples/sec: 21.44 - lr: 0.000013
2022-01-05 23:40:27,609 epoch 8 - iter 1704/2138 - loss 0.03723524 - samples/sec: 21.20 - lr: 0.000012
2022-01-05 23:40:46,619 epoch 8 - iter 1917/2138 - loss 0.03633406 - samples/sec: 22.74 - lr: 0.000012
2022-01-05 23:41:06,022 epoch 8 - iter 2130/2138 - loss 0.03623926 - samples/sec: 22.25 - lr: 0.000011
2022-01-05 23:41:07,063 ----------------------------------------------------------------------------------------------------
2022-01-05 23:41:07,063 EPOCH 8 done: loss 77.4825 - lr 0.0000111
2022-01-05 23:41:25,053 DEV : loss 0.22630879282951355 - f1-score (micro avg)  0.8906
2022-01-05 23:41:26,565 BAD EPOCHS (no improvement): 4
2022-01-05 23:41:26,565 ----------------------------------------------------------------------------------------------------
2022-01-05 23:41:58,676 epoch 9 - iter 213/2138 - loss 0.01068994 - samples/sec: 20.57 - lr: 0.000011
2022-01-05 23:42:20,063 epoch 9 - iter 426/2138 - loss 0.01344233 - samples/sec: 20.28 - lr: 0.000010
2022-01-05 23:42:40,645 epoch 9 - iter 639/2138 - loss 0.01685915 - samples/sec: 20.99 - lr: 0.000009
2022-01-05 23:43:01,150 epoch 9 - iter 852/2138 - loss 0.01970331 - samples/sec: 21.17 - lr: 0.000009
2022-01-05 23:43:21,765 epoch 9 - iter 1065/2138 - loss 0.01971913 - samples/sec: 21.04 - lr: 0.000008
2022-01-05 23:43:40,953 epoch 9 - iter 1278/2138 - loss 0.01994089 - samples/sec: 22.49 - lr: 0.000008
2022-01-05 23:44:00,871 epoch 9 - iter 1491/2138 - loss 0.02105452 - samples/sec: 21.73 - lr: 0.000007
2022-01-05 23:44:20,725 epoch 9 - iter 1704/2138 - loss 0.02079326 - samples/sec: 21.80 - lr: 0.000007
2022-01-05 23:44:40,802 epoch 9 - iter 1917/2138 - loss 0.01991178 - samples/sec: 21.55 - lr: 0.000006
2022-01-05 23:45:00,694 epoch 9 - iter 2130/2138 - loss 0.01902076 - samples/sec: 21.73 - lr: 0.000006
2022-01-05 23:45:01,783 ----------------------------------------------------------------------------------------------------
2022-01-05 23:45:01,783 EPOCH 9 done: loss 40.7005 - lr 0.0000056
2022-01-05 23:45:19,156 DEV : loss 0.21481601893901825 - f1-score (micro avg)  0.8947
2022-01-05 23:45:20,767 BAD EPOCHS (no improvement): 4
2022-01-05 23:45:20,768 ----------------------------------------------------------------------------------------------------
2022-01-05 23:45:50,929 epoch 10 - iter 213/2138 - loss 0.01354144 - samples/sec: 22.34 - lr: 0.000005
2022-01-05 23:46:11,165 epoch 10 - iter 426/2138 - loss 0.01265123 - samples/sec: 21.38 - lr: 0.000004
2022-01-05 23:46:30,588 epoch 10 - iter 639/2138 - loss 0.01237722 - samples/sec: 22.29 - lr: 0.000004
2022-01-05 23:46:50,486 epoch 10 - iter 852/2138 - loss 0.01214318 - samples/sec: 21.70 - lr: 0.000003
2022-01-05 23:47:10,524 epoch 10 - iter 1065/2138 - loss 0.01107514 - samples/sec: 21.68 - lr: 0.000003
2022-01-05 23:47:30,586 epoch 10 - iter 1278/2138 - loss 0.01010205 - samples/sec: 21.58 - lr: 0.000002
2022-01-05 23:47:51,044 epoch 10 - iter 1491/2138 - loss 0.01098466 - samples/sec: 21.17 - lr: 0.000002
2022-01-05 23:48:10,948 epoch 10 - iter 1704/2138 - loss 0.01130165 - samples/sec: 21.75 - lr: 0.000001
2022-01-05 23:48:30,345 epoch 10 - iter 1917/2138 - loss 0.01134772 - samples/sec: 22.29 - lr: 0.000001
2022-01-05 23:48:50,291 epoch 10 - iter 2130/2138 - loss 0.01080190 - samples/sec: 21.66 - lr: 0.000000
2022-01-05 23:48:51,382 ----------------------------------------------------------------------------------------------------
2022-01-05 23:48:51,382 EPOCH 10 done: loss 23.0165 - lr 0.0000000
2022-01-05 23:49:08,900 DEV : loss 0.21795277297496796 - f1-score (micro avg)  0.8944
2022-01-05 23:49:10,314 BAD EPOCHS (no improvement): 4
2022-01-05 23:49:10,789 ----------------------------------------------------------------------------------------------------
2022-01-05 23:49:10,789 Testing using last state of model ...
2022-01-05 23:49:23,812 0.4442822119164048	0.8870250325495753	0.8878184252709931
2022-01-05 23:49:23,812 AVG: mse: 0.4443 - mae: 0.4015 - pearson: 0.8878 - spearman: 0.8870
2022-01-05 23:49:23,812 ----------------------------------------------------------------------------------------------------
